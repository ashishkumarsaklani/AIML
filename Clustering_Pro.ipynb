{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyNdYHfwihMsV90ys91ofngA",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/ashishkumarsaklani/AIML/blob/main/Clustering_Pro.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#cluster project\n",
        "\n",
        "#importing Libraries"
      ],
      "metadata": {
        "id": "Vxp2A28cGt75"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 607,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "brxzXIX9GsIi",
        "outputId": "8855766b-3f90-4123-febd-7bf3c200f166"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Package stopwords is already up-to-date!\n",
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Package punkt is already up-to-date!\n"
          ]
        }
      ],
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "\n",
        "\n",
        "\n",
        "from sklearn.impute import KNNImputer\n",
        "from sklearn.preprocessing import LabelEncoder,StandardScaler,MinMaxScaler,PolynomialFeatures\n",
        "from sklearn.metrics.pairwise import pairwise_kernels\n",
        "from sklearn.mixture import GaussianMixture\n",
        "from sklearn.cluster import KMeans,SpectralClustering\n",
        "\n",
        "\n",
        "from scipy.cluster.hierarchy import dendrogram, linkage, fcluster\n",
        "from mpl_toolkits.mplot3d import Axes3D\n",
        "import plotly.express as px\n",
        "\n",
        "from nltk.corpus import stopwords\n",
        "from nltk.tokenize import word_tokenize\n",
        "from nltk.stem import PorterStemmer\n",
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "from collections import Counter\n",
        "import nltk\n",
        "nltk.download('stopwords')\n",
        "import nltk\n",
        "nltk.download('punkt')\n",
        "\n",
        "import re\n",
        "import itertools\n",
        "import warnings\n",
        "warnings.filterwarnings('ignore')"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#custom functions"
      ],
      "metadata": {
        "id": "3Nc31-eyHRDa"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "class TextCategorizer:\n",
        "    def __init__(self, cluster_count=50):\n",
        "        self.cluster_count = cluster_count\n",
        "        self.stemmer = PorterStemmer()\n",
        "        self.vectorizer = TfidfVectorizer()\n",
        "        self.kmeans = KMeans(n_clusters=self.cluster_count)\n",
        "        self.representative_titles = {}\n",
        "\n",
        "    def normalize_text(self, text):\n",
        "        # Normalizing text: correcting spelling mistakes, converting to lowercase, and standardizing casing\n",
        "        return text.lower()\n",
        "\n",
        "#    def fit(self, text_column):\n",
        "#        normalized_text = text_column.apply(self.normalize_text)\n",
        "#        self.vectorizer.fit(normalized_text)\n",
        "#        X = self.vectorizer.transform(normalized_text)\n",
        "#        self.clusters = self.kmeans.fit_predict(X)\n",
        "#        self.representative_titles = self._find_representative_titles(text_column)\n",
        "\n",
        "    def fit(self, text_column):\n",
        "        normalized_text_column = text_column.copy()  # Make a copy of the original text column\n",
        "        normalized_text_column = normalized_text_column.apply(self.normalize_text)\n",
        "        self.vectorizer.fit(normalized_text_column)\n",
        "        X = self.vectorizer.transform(normalized_text_column)\n",
        "        self.clusters = self.kmeans.fit_predict(X)\n",
        "        self.representative_titles = self._find_representative_titles(text_column)\n",
        "\n",
        "    def _find_representative_titles(self, text_column):\n",
        "        top_job_titles = list(text_column.value_counts().head(self.cluster_count).index)# correction made here self.cluster_count *2\n",
        "        representative_titles = {}\n",
        "        for cluster_id in range(self.cluster_count):\n",
        "            cluster_indices = [i for i, label in enumerate(self.clusters) if label == cluster_id]\n",
        "            cluster_job_titles = text_column.iloc[cluster_indices]\n",
        "            # Filter out non-English or meaningless job titles\n",
        "            cluster_job_titles = cluster_job_titles[cluster_job_titles.isin(top_job_titles)]\n",
        "            # Select the most common job title in the cluster\n",
        "            common_job_title = cluster_job_titles.value_counts().index[0] if not cluster_job_titles.empty else 'Others'\n",
        "            representative_titles[cluster_id] = common_job_title\n",
        "        return representative_titles\n",
        "\n",
        "    def predict(self, text_column):\n",
        "        normalized_text = text_column.apply(self.normalize_text)\n",
        "        X = self.vectorizer.transform(normalized_text)\n",
        "        predicted_clusters = self.kmeans.predict(X)\n",
        "        return [self.representative_titles[cluster_id] for cluster_id in predicted_clusters]"
      ],
      "metadata": {
        "id": "of1ZXkksEsTU"
      },
      "execution_count": 608,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "class TextCategorizer2:\n",
        "    def __init__(self, cluster_count=30):\n",
        "        self.cluster_count = cluster_count\n",
        "        self.stemmer = PorterStemmer()\n",
        "        self.vectorizer = TfidfVectorizer()\n",
        "        self.kmeans = KMeans(n_clusters=self.cluster_count)\n",
        "        self.representative_titles = {}\n",
        "\n",
        "    def normalize_text(self, text):\n",
        "        # Normalizing text: correcting spelling mistakes, converting to lowercase, and standardizing casing\n",
        "        return text.lower()\n",
        "\n",
        "#    def fit(self, text_column):\n",
        "#        normalized_text = text_column.apply(self.normalize_text)\n",
        "#        self.vectorizer.fit(normalized_text)\n",
        "#        X = self.vectorizer.transform(normalized_text)\n",
        "#        self.clusters = self.kmeans.fit_predict(X)\n",
        "#        self.representative_titles = self._find_representative_titles(text_column)\n",
        "\n",
        "    def fit(self, text_column):\n",
        "        normalized_text_column = text_column.copy()  # Make a copy of the original text column\n",
        "        normalized_text_column = normalized_text_column.apply(self.normalize_text)\n",
        "        self.vectorizer.fit(normalized_text_column)\n",
        "        X = self.vectorizer.transform(normalized_text_column)\n",
        "        self.clusters = self.kmeans.fit_predict(X)\n",
        "        self.representative_titles = self._find_representative_titles(text_column)\n",
        "\n",
        "    def _find_representative_titles(self, text_column):\n",
        "        top_job_titles = list(text_column.value_counts().head(self.cluster_count * 2).index)\n",
        "        representative_titles = {}\n",
        "        for cluster_id in range(self.cluster_count):\n",
        "            cluster_indices = [i for i, label in enumerate(self.clusters) if label == cluster_id]\n",
        "            cluster_job_titles = text_column.iloc[cluster_indices]\n",
        "            # Filter out non-English or meaningless job titles\n",
        "            cluster_job_titles = cluster_job_titles[cluster_job_titles.isin(top_job_titles)]\n",
        "            # Select the most common job title in the cluster\n",
        "            common_job_title = cluster_job_titles.value_counts().index[0] if not cluster_job_titles.empty else 'Others'\n",
        "            representative_titles[cluster_id] = common_job_title\n",
        "        return representative_titles\n",
        "\n",
        "    def predict(self, text_column):\n",
        "        normalized_text = text_column.apply(self.normalize_text)\n",
        "        X = self.vectorizer.transform(normalized_text)\n",
        "        predicted_clusters = self.kmeans.predict(X)\n",
        "        return [self.representative_titles[cluster_id] for cluster_id in predicted_clusters]"
      ],
      "metadata": {
        "id": "_GnAg5BtmeAO"
      },
      "execution_count": 609,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Normalisation of text string and preprocessor for nltk\n",
        "\n",
        "#import pandas as pd\n",
        "#import re\n",
        "#from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "#from sklearn.cluster import KMeans\n",
        "#from nltk.corpus import stopwords\n",
        "#from nltk.stem import PorterStemmer\n",
        "#from nltk.tokenize import word_tokenize\n",
        "\n",
        "class TextCategorizer_clustering:\n",
        "    def __init__(self, n_clusters=90):\n",
        "        self.n_clusters = n_clusters\n",
        "        self.stop_words = set(stopwords.words('english'))\n",
        "        self.stemmer = PorterStemmer()\n",
        "        self.vectorizer = TfidfVectorizer()\n",
        "        self.kmeans = KMeans(n_clusters=self.n_clusters)\n",
        "        self.representative_titles = {}\n",
        "\n",
        "    def preprocess_text(self, text):\n",
        "        text = text.lower()  # Convert to lowercase\n",
        "        text = re.sub(r'[^\\w\\s]', '', text)  # Remove punctuation\n",
        "        tokens = word_tokenize(text)\n",
        "        tokens = [self.stemmer.stem(token) for token in tokens if token not in self.stop_words]\n",
        "\n",
        "\n",
        "        #print (tokens)# for debug\n",
        "        return ' '.join(tokens)\n",
        "\n",
        "    def fit(self, text_column):\n",
        "        text_data = pd.DataFrame(text_column)\n",
        "        text_data = text_data.fillna('Not Disclosed')  # Fill NaN values\n",
        "\n",
        "        #print(1,\"Text data:\")\n",
        "        #print(text_data.head())  # Print the first few rows of text data to verify it's correctly loaded\n",
        "\n",
        "        preprocessed_text = text_data.iloc[:, 0].apply(self.preprocess_text)\n",
        "\n",
        "        #print(2,\"Preprocessed text:\")\n",
        "        #print(preprocessed_text.head())  # Print the first few preprocessed texts to verify preprocessing is working\n",
        "\n",
        "        X = self.vectorizer.fit_transform(preprocessed_text)\n",
        "\n",
        "        #print(3,\"Vocabulary:\")\n",
        "        #print(self.vectorizer.get_feature_names()[:10])  # Print the first few feature names to verify vocabulary creation\n",
        "\n",
        "        self.clusters = self.kmeans.fit_predict(X)\n",
        "        self.representative_titles = self._find_representative_titles(text_data.iloc[:,0])\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "    def _find_representative_titles(self, text_data):\n",
        "        representative_titles = {}\n",
        "        for cluster_id in range(self.n_clusters):\n",
        "            cluster_indices = [i for i, label in enumerate(self.clusters) if label == cluster_id]\n",
        "            common_job_titles = text_data.iloc[cluster_indices].value_counts().index[0]  # Get most common title\n",
        "            representative_titles[cluster_id] = common_job_titles\n",
        "        return representative_titles\n",
        "\n",
        "    def predict(self, text_column):\n",
        "        return text_column.apply(lambda x: self._assign_clean_title(x))\n",
        "\n",
        "    def _assign_clean_title(self, text):\n",
        "        if pd.isna(text):\n",
        "            return 'Not Disclosed'\n",
        "        try:\n",
        "            cluster_id = self.clusters[list(self.vectorizer.transform([text]))[0].argmax()]\n",
        "            return self.representative_titles.get(cluster_id, 'Others')\n",
        "        except ValueError:\n",
        "            return 'Other'"
      ],
      "metadata": {
        "id": "zBRz2BZZGtA_"
      },
      "execution_count": 610,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class IQROutlierDetector:\n",
        "\n",
        "  def __init__(self, data, column_name):\n",
        "\n",
        "    self.data = data\n",
        "    self.column_name = column_name\n",
        "\n",
        "  def detect_outliers(self):\n",
        "\n",
        "\n",
        "    if not pd.api.types.is_numeric_dtype(self.data[self.column_name]):\n",
        "      #print(f\"Warning: Column '{self.column_name}' is not numerical. Skipping outlier detection.\")\n",
        "      return None  # Indicate non-numerical column\n",
        "\n",
        "    q1 = self.data[self.column_name].quantile(0.25)\n",
        "    q3 = self.data[self.column_name].quantile(0.75)\n",
        "    iqr = q3 - q1\n",
        "    lower_bound = q1 - 1.5 * iqr\n",
        "    upper_bound = q3 + 1.5 * iqr\n",
        "    outliers = self.data[(self.data[self.column_name] < lower_bound) | (self.data[self.column_name] > upper_bound)].index.tolist()\n",
        "    return outliers\n",
        "\n",
        "  def get_outlier_count(self):\n",
        "\n",
        "    outliers = self.detect_outliers()\n",
        "    if outliers is None:\n",
        "        return 0  # Handle case of non-numerical column\n",
        "    else:\n",
        "        return len(outliers)\n",
        "\n",
        "  def get_outlier_percentage(self):\n",
        "\n",
        "    outlier_count = self.get_outlier_count()\n",
        "    total_data_points = len(self.data)\n",
        "    if total_data_points > 0:\n",
        "        return (outlier_count / total_data_points) * 100\n",
        "    else:\n",
        "        return 0  # Handle case of empty data\n",
        "\n",
        "  def drop_outliers(self):\n",
        "\n",
        "    outliers = self.detect_outliers()\n",
        "    if outliers is None:\n",
        "        return self.data  # Return original data if column is not numerical\n",
        "    else:\n",
        "        return self.data.drop(outliers)"
      ],
      "metadata": {
        "id": "lsIvKzcSRz9G"
      },
      "execution_count": 611,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#custom Encoder\n",
        "class CustomEncoder:\n",
        "    def __init__(self):\n",
        "        self.encoding_dict = {}  # Dictionary to store encoding mappings\n",
        "        self.counter = 1  # Counter to assign integer encoding values\n",
        "\n",
        "    def fit_transform(self, series):\n",
        "        encoded_list = []  # List to store encoded values\n",
        "\n",
        "        # Count the frequency of each item in the series\n",
        "        frequency_count = series.value_counts().sort_values(ascending=False)\n",
        "\n",
        "        # Iterate through the series based on frequency count\n",
        "        for item in series:\n",
        "            if pd.isna(item):\n",
        "                encoded_list.append(item)\n",
        "            elif item not in self.encoding_dict:\n",
        "                self.encoding_dict[item] = self.counter\n",
        "                self.counter += 1\n",
        "                encoded_list.append(self.encoding_dict[item])\n",
        "            else:\n",
        "                encoded_list.append(self.encoding_dict[item])\n",
        "\n",
        "        return encoded_list, self.encoding_dict\n",
        "\n",
        "    def inverse_transform(self, encoded_list, encoding_dict):\n",
        "        reverse_dict = {v: k for k, v in encoding_dict.items()}  # Reverse the dictionary for decoding\n",
        "        return [reverse_dict[encoded_value] if encoded_value in reverse_dict else encoded_value for encoded_value in encoded_list]"
      ],
      "metadata": {
        "id": "DPN5x8uAJIg7"
      },
      "execution_count": 612,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "from sklearn.impute import KNNImputer\n",
        "\n",
        "class KNNBatchImputer(KNNImputer):\n",
        "    def __init__(self, batch_size=500, n_neighbors=5):\n",
        "        super().__init__(n_neighbors=n_neighbors)  # Inherit from KNNImputer\n",
        "        self.batch_size = batch_size\n",
        "\n",
        "    def fit_transform(self, df):\n",
        "        data_filled = pd.DataFrame()\n",
        "        num_batches = len(df) // self.batch_size + (1 if len(df) % self.batch_size != 0 else 0)\n",
        "\n",
        "        for i in range(num_batches):\n",
        "            start_idx = i * self.batch_size\n",
        "            end_idx = min((i + 1) * self.batch_size, len(df))\n",
        "            batch_data = df.iloc[start_idx:end_idx].copy()\n",
        "\n",
        "            imputer = KNNImputer(n_neighbors=self.n_neighbors)  # Create KNNImputer for each batch\n",
        "            batch_data_filled = pd.DataFrame(imputer.fit_transform(batch_data), columns=batch_data.columns)\n",
        "\n",
        "            data_filled = pd.concat([data_filled, batch_data_filled], ignore_index=True)\n",
        "\n",
        "        return data_filled\n",
        "\n",
        "# Example usage\n",
        "#df_filled = KNNImputerWithBatching(batch_size=500, n_neighbors=5).fit_transform(df.copy())  # Avoid modifying original df\n",
        "#print(df_filled.isnull().sum())  # Check for remaining missing values"
      ],
      "metadata": {
        "id": "AiHjOAk1fXJD"
      },
      "execution_count": 613,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "import pandas as pd\n",
        "from sklearn.impute import KNNImputer\n",
        "\n",
        "class KNNImputerWithCustomEncoder1:\n",
        "    def __init__(self, batch_size=500, n_neighbors=5, custom_encoder=None):\n",
        "        self.batch_size = batch_size\n",
        "        self.n_neighbors = n_neighbors\n",
        "        self.custom_encoder = custom_encoder\n",
        "        self.encoding_dicts = {}\n",
        "\n",
        "    def fit_transform(self, df):\n",
        "        data_filled = pd.DataFrame()\n",
        "        num_batches = len(df) // self.batch_size + (1 if len(df) % self.batch_size != 0 else 0)\n",
        "\n",
        "        for i in range(num_batches):\n",
        "            start_idx = i * self.batch_size\n",
        "            end_idx = min((i + 1) * self.batch_size, len(df))\n",
        "            batch_data = df.iloc[start_idx:end_idx].copy()\n",
        "\n",
        "            self._encode_unsupported_types(batch_data)\n",
        "            imputer = KNNImputer(n_neighbors=self.n_neighbors)\n",
        "            batch_data_filled = pd.DataFrame(imputer.fit_transform(batch_data), columns=batch_data.columns)\n",
        "            self._decode_unsupported_types(batch_data_filled)\n",
        "\n",
        "            data_filled = pd.concat([data_filled, batch_data_filled], ignore_index=True)\n",
        "\n",
        "        return data_filled\n",
        "\n",
        "    def _encode_unsupported_types(self, df):\n",
        "        for col in df.columns:\n",
        "            if df[col].dtype not in ('int64', 'float64'):\n",
        "                if df[col].dtype == 'object':\n",
        "                    df[col], encoding_dict = self.custom_encoder.fit_transform(df[col])\n",
        "                    self.encoding_dicts[col] = encoding_dict\n",
        "\n",
        "    def _decode_unsupported_types(self, df):\n",
        "        for col in df.columns:\n",
        "            if col in self.encoding_dicts:\n",
        "                df[col] = self.custom_encoder.inverse_transform(   np.round(np.array(df[col])).astype('int64')  , self.encoding_dicts[col])\n",
        "\n",
        "\n",
        "# Example usage:\n",
        "# custom_encoder = CustomEncoder()  # Initialize your custom encoder\n",
        "# imputer = KNNImputerWithCustomEncoder(custom_encoder=custom_encoder)\n",
        "# df_filled = imputer.fit_transform(df)\n",
        "# print(df_filled)"
      ],
      "metadata": {
        "id": "X_Gc9LYekujq"
      },
      "execution_count": 614,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def check_missing(data):\n",
        "   print( df.iloc[-3:,-2:])\n",
        "   for col in data.columns:\n",
        "      Mi = data[col].isna().sum()\n",
        "      print(f\"{Mi}  : {col}\")"
      ],
      "metadata": {
        "id": "8z-TYAXJutVD"
      },
      "execution_count": 615,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Importing Data"
      ],
      "metadata": {
        "id": "gLP7WWbwSJHw"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "url ='https://d2beiqkhq929f0.cloudfront.net/public_assets/assets/000/002/856/original/scaler_clustering.csv'\n",
        "df = pd.read_csv(url)\n",
        "df.info()"
      ],
      "metadata": {
        "id": "SbdaoEwnSFeh",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "6c95e909-f859-4e21-c8ee-0d69014b8af9"
      },
      "execution_count": 616,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "<class 'pandas.core.frame.DataFrame'>\n",
            "RangeIndex: 205843 entries, 0 to 205842\n",
            "Data columns (total 7 columns):\n",
            " #   Column            Non-Null Count   Dtype  \n",
            "---  ------            --------------   -----  \n",
            " 0   Unnamed: 0        205843 non-null  int64  \n",
            " 1   company_hash      205799 non-null  object \n",
            " 2   email_hash        205843 non-null  object \n",
            " 3   orgyear           205757 non-null  float64\n",
            " 4   ctc               205843 non-null  int64  \n",
            " 5   job_position      153279 non-null  object \n",
            " 6   ctc_updated_year  205843 non-null  float64\n",
            "dtypes: float64(2), int64(2), object(3)\n",
            "memory usage: 11.0+ MB\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Renaming columns for my convenience"
      ],
      "metadata": {
        "id": "DcXa2RBgSx4m"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "df.rename(columns={\n",
        "'Unnamed: 0':'sn',\n",
        "'company_hash':'company',\n",
        "'email_hash':'email',\n",
        "'orgyear':'j_year',\n",
        "'job_position':'position',\n",
        "'ctc_updated_year':'i_year'\n",
        "},inplace=True)"
      ],
      "metadata": {
        "id": "YVz2lAJsS3YW"
      },
      "execution_count": 617,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Checking Duplicates"
      ],
      "metadata": {
        "id": "r2jsGAXOV5kX"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#checking if there are duplicate and total\n",
        "df.duplicated().sum()"
      ],
      "metadata": {
        "id": "4lCjBZjhV_Yb",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "54dba0fd-58e1-4cc6-aef6-0838e654b928"
      },
      "execution_count": 618,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "0"
            ]
          },
          "metadata": {},
          "execution_count": 618
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#No reason to remove duplicate from seprate columns\n",
        "for col in df.columns:\n",
        "   dup = df[col].duplicated().sum()\n",
        "   print(f\"{dup} :{col}\")"
      ],
      "metadata": {
        "id": "YdbXCiGrWahT",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "04e7cfd1-d396-45ea-f13e-2456e212936e"
      },
      "execution_count": 619,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "0 :sn\n",
            "168543 :company\n",
            "52400 :email\n",
            "205765 :j_year\n",
            "202483 :ctc\n",
            "204826 :position\n",
            "205836 :i_year\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Unique Values"
      ],
      "metadata": {
        "id": "FD611Hl0WvOL"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "df.nunique()"
      ],
      "metadata": {
        "id": "BSl3-lOAW1c4",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "065f8a4e-97c4-4313-8028-e5cf4d27cab8"
      },
      "execution_count": 620,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "sn          205843\n",
              "company      37299\n",
              "email       153443\n",
              "j_year          77\n",
              "ctc           3360\n",
              "position      1016\n",
              "i_year           7\n",
              "dtype: int64"
            ]
          },
          "metadata": {},
          "execution_count": 620
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Outlier detection and removal"
      ],
      "metadata": {
        "id": "0bdqeGcOY9Vo"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "for col in df.columns:\n",
        "  detector = IQROutlierDetector(df, col)\n",
        "  outlier_indices = detector.detect_outliers()\n",
        "  #print(\"Outlier indices:\", outlier_indices)\n",
        "\n",
        "  outlier_count = detector.get_outlier_count()\n",
        "  outlier_percentage = round(detector.get_outlier_percentage(),2)\n",
        "  if detector.get_outlier_count() > 0 :\n",
        "    print(f\"{col} Outlier count:\", outlier_count)\n",
        "    print(f\"{col} Outlier Percentage:\", outlier_percentage, \"%\")\n",
        "    df = detector.drop_outliers()"
      ],
      "metadata": {
        "id": "ZpsWToMeZEcq",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "d55fa5c6-ccef-4e06-892d-aa32aec402d0"
      },
      "execution_count": 621,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "j_year Outlier count: 7764\n",
            "j_year Outlier Percentage: 3.77 %\n",
            "ctc Outlier count: 12366\n",
            "ctc Outlier Percentage: 6.24 %\n",
            "i_year Outlier count: 2610\n",
            "i_year Outlier Percentage: 1.41 %\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Checking for Missing Values"
      ],
      "metadata": {
        "id": "I2kwX55STP5D"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#Now checking for missing values\n",
        "check_missing(df)"
      ],
      "metadata": {
        "id": "LGyj0vruTa3n",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "2d2e93bf-90fb-4b69-952f-dc2ed9e4513d"
      },
      "execution_count": 622,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "       position  i_year\n",
            "205839      NaN  2020.0\n",
            "205840      NaN  2021.0\n",
            "205842      NaN  2016.0\n",
            "0  : sn\n",
            "42  : company\n",
            "0  : email\n",
            "77  : j_year\n",
            "0  : ctc\n",
            "48245  : position\n",
            "0  : i_year\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#removing missing values from company and J_year as the account is low\n",
        "df.dropna(subset= ['company','j_year'],inplace=True,axis=0)"
      ],
      "metadata": {
        "id": "9j2NwCAhTerd"
      },
      "execution_count": 623,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Now checking for missing values\n",
        "check_missing(df)"
      ],
      "metadata": {
        "id": "MPzqrMZFf7gg",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "98522d67-1acf-4928-84db-b7feb66a8e63"
      },
      "execution_count": 624,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "       position  i_year\n",
            "205839      NaN  2020.0\n",
            "205840      NaN  2021.0\n",
            "205842      NaN  2016.0\n",
            "0  : sn\n",
            "0  : company\n",
            "0  : email\n",
            "0  : j_year\n",
            "0  : ctc\n",
            "48195  : position\n",
            "0  : i_year\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Changing datatype"
      ],
      "metadata": {
        "id": "c_Y50cE6mdog"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "df['j_year']=df['j_year'].astype('int64')\n",
        "df['i_year']=df['i_year'].astype('int64')"
      ],
      "metadata": {
        "id": "IQukTxbWmho0"
      },
      "execution_count": 625,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "#input_series = df['position']\n",
        "#df['position'], pos_dict = encoder.fit_transform(input_series)\n",
        "#input_series = df['company']\n",
        "#df['company'], com_dict = encoder.fit_transform(input_series)\n",
        "#input_series = df['email']\n",
        "#df['email'], ema_dict = encoder.fit_transform(input_series)"
      ],
      "metadata": {
        "id": "zfNi0mKPmKWB"
      },
      "execution_count": 626,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "encoder = CustomEncoder()  # Create an instance of CustomEncoder\n",
        "\n",
        "imputer = KNNImputerWithCustomEncoder1(custom_encoder = encoder)\n",
        "df_filled = imputer.fit_transform(df)\n",
        "df = df_filled\n",
        "check_missing(df)"
      ],
      "metadata": {
        "id": "6yYt06lxf9wY",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "aa220733-12e8-4766-9b88-b07d489e6341"
      },
      "execution_count": 627,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "                 position  i_year\n",
            "182981   Backend Engineer  2020.0\n",
            "182982       Data Analyst  2021.0\n",
            "182983  Frontend Engineer  2016.0\n",
            "0  : sn\n",
            "0  : company\n",
            "0  : email\n",
            "0  : j_year\n",
            "0  : ctc\n",
            "0  : position\n",
            "0  : i_year\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Create an instance of the class\n",
        "categorizer = TextCategorizer()\n",
        "\n",
        "# Fit the model on the text column\n",
        "categorizer.fit(df['position'])\n",
        "\n",
        "\n",
        "# Predict categories for new text data\n",
        "df['predicted'] = categorizer.predict(df['position'])\n",
        "\n",
        "# Print the results\n",
        "#print(df[['position', 'predicted']].head(30))"
      ],
      "metadata": {
        "id": "o7sxqgVJ8-p3"
      },
      "execution_count": 628,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df['predicted'].value_counts().head(25)"
      ],
      "metadata": {
        "id": "HJHJiMwG6F8V",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "daaed9c1-83a6-4a8d-ef9f-13b5fadefc35"
      },
      "execution_count": 629,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "predicted\n",
              "Backend Engineer             45125\n",
              "FullStack Engineer           27650\n",
              "Other                        16631\n",
              "Frontend Engineer            15125\n",
              "Data Analyst                  9886\n",
              "iOS Engineer                  9470\n",
              "Engineering Leadership        7417\n",
              "Data Scientist                7379\n",
              "QA Engineer                   6521\n",
              "Android Engineer              5888\n",
              "zvcxv rxet wvqt               5497\n",
              "SDET                          4709\n",
              "Devops Engineer               4166\n",
              "Engineering Intern            4155\n",
              "Support Engineer              3426\n",
              "Research Engineers            1368\n",
              "Product Designer              1014\n",
              "Product Manager                886\n",
              "Backend Architect              584\n",
              "Non Coder                      560\n",
              "Others                         551\n",
              "Database Administrator         461\n",
              "Senior  Software Engineer      396\n",
              "Program Manager                388\n",
              "qgjvr tzextra                  350\n",
              "Name: count, dtype: int64"
            ]
          },
          "metadata": {},
          "execution_count": 629
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(\n",
        "df[['position','predicted']][-10:])\n",
        "print(f\"unique position count:{df['position'].nunique()},unique predicted count: {df['predicted'].nunique()}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "yi4A0TUp4sv5",
        "outputId": "e47bb4f7-5f4e-4026-dbc2-07093695a984"
      },
      "execution_count": 630,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "                      position               predicted\n",
            "182974        Backend Engineer        Backend Engineer\n",
            "182975            Data Analyst            Data Analyst\n",
            "182976            Data Analyst            Data Analyst\n",
            "182977        Backend Engineer        Backend Engineer\n",
            "182978        Backend Engineer        Backend Engineer\n",
            "182979  Engineering Leadership  Engineering Leadership\n",
            "182980            Data Analyst            Data Analyst\n",
            "182981        Backend Engineer        Backend Engineer\n",
            "182982            Data Analyst            Data Analyst\n",
            "182983       Frontend Engineer       Frontend Engineer\n",
            "unique position count:2028,unique predicted count: 43\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#filtering doubtful data\n",
        "data where increment was given before joining"
      ],
      "metadata": {
        "id": "k4y_ESy8TPDh"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "\n",
        "df = df[~(df['i_year'] < df['j_year'])]\n",
        "\n",
        "#df[df['i_year'] < df['j_year']].iloc[:5,-5:]\n",
        "\n",
        "df = df[~(df['position'] != df['predicted'])]"
      ],
      "metadata": {
        "id": "dxj4ZvZhWzX2"
      },
      "execution_count": 641,
      "outputs": []
    }
  ]
}