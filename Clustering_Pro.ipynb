{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyNnfP9+R7Pp5xzudjeYQBPv",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/ashishkumarsaklani/AIML/blob/main/Clustering_Pro.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#cluster project\n",
        "\n",
        "#importing Libraries"
      ],
      "metadata": {
        "id": "Vxp2A28cGt75"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 66,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "brxzXIX9GsIi",
        "outputId": "5eb840b2-1bca-4aab-94ee-82735bf34a69"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Package stopwords is already up-to-date!\n",
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Package punkt is already up-to-date!\n"
          ]
        }
      ],
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "\n",
        "\n",
        "\n",
        "from sklearn.impute import KNNImputer\n",
        "from sklearn.preprocessing import LabelEncoder,StandardScaler,MinMaxScaler,PolynomialFeatures\n",
        "from sklearn.metrics.pairwise import pairwise_kernels\n",
        "from sklearn.mixture import GaussianMixture\n",
        "from sklearn.cluster import KMeans,SpectralClustering\n",
        "\n",
        "\n",
        "from scipy.cluster.hierarchy import dendrogram, linkage, fcluster\n",
        "from mpl_toolkits.mplot3d import Axes3D\n",
        "import plotly.express as px\n",
        "\n",
        "from nltk.corpus import stopwords\n",
        "from nltk.tokenize import word_tokenize\n",
        "from nltk.stem import PorterStemmer\n",
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "from collections import Counter\n",
        "import nltk\n",
        "nltk.download('stopwords')\n",
        "import nltk\n",
        "nltk.download('punkt')\n",
        "\n",
        "import re\n",
        "import itertools\n",
        "import warnings\n",
        "warnings.filterwarnings('ignore')"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#custom functions"
      ],
      "metadata": {
        "id": "3Nc31-eyHRDa"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#df[columns=1]"
      ],
      "metadata": {
        "id": "of1ZXkksEsTU"
      },
      "execution_count": 64,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Normalisation of text string and preprocessor for nltk\n",
        "\n",
        "#import pandas as pd\n",
        "#import re\n",
        "#from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "#from sklearn.cluster import KMeans\n",
        "#from nltk.corpus import stopwords\n",
        "#from nltk.stem import PorterStemmer\n",
        "#from nltk.tokenize import word_tokenize\n",
        "\n",
        "class TextCategorizer:\n",
        "    def __init__(self, n_clusters=50):\n",
        "        self.n_clusters = n_clusters\n",
        "        self.stop_words = set(stopwords.words('english'))\n",
        "        self.stemmer = PorterStemmer()\n",
        "        self.vectorizer = TfidfVectorizer()\n",
        "        self.kmeans = KMeans(n_clusters=self.n_clusters)\n",
        "        self.representative_titles = {}\n",
        "\n",
        "    def preprocess_text(self, text):\n",
        "        text = text.lower()  # Convert to lowercase\n",
        "        text = re.sub(r'[^\\w\\s]', '', text)  # Remove punctuation\n",
        "        tokens = word_tokenize(text)\n",
        "        tokens = [self.stemmer.stem(token) for token in tokens if token not in self.stop_words]\n",
        "\n",
        "\n",
        "        print (tokens)# for debug\n",
        "        return ' '.join(tokens)\n",
        "\n",
        "    def fit(self, text_column):\n",
        "        text_data = pd.DataFrame(text_column)\n",
        "        text_data = text_data.fillna('Not Disclosed')  # Fill NaN values\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "        print (text_data.iloc[0:])\n",
        "        preprocessed_text = text_data.iloc[0].apply(self.preprocess_text)\n",
        "        X = self.vectorizer.fit_transform(preprocessed_text)\n",
        "\n",
        "        self.clusters = self.kmeans.fit_predict(X)\n",
        "        self.representative_titles = self._find_representative_titles(text_data[0])\n",
        "\n",
        "\n",
        "    def _find_representative_titles(self, text_data):\n",
        "        representative_titles = {}\n",
        "        for cluster_id in range(self.n_clusters):\n",
        "            cluster_indices = [i for i, label in enumerate(self.clusters) if label == cluster_id]\n",
        "            common_job_titles = text_data.iloc[cluster_indices].value_counts().index[0]  # Get most common title\n",
        "            representative_titles[cluster_id] = common_job_titles\n",
        "        return representative_titles\n",
        "\n",
        "    def predict(self, text_column):\n",
        "        return text_column.apply(lambda x: self._assign_clean_title(x))\n",
        "\n",
        "    def _assign_clean_title(self, text):\n",
        "        if pd.isna(text):\n",
        "            return 'Not Disclosed'\n",
        "        try:\n",
        "            cluster_id = self.clusters[list(self.vectorizer.transform([text]))[0].argmax()]\n",
        "            return self.representative_titles.get(cluster_id, 'Others')\n",
        "        except ValueError:\n",
        "            return 'Other'"
      ],
      "metadata": {
        "id": "zBRz2BZZGtA_"
      },
      "execution_count": 67,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class IQROutlierDetector:\n",
        "\n",
        "  def __init__(self, data, column_name):\n",
        "\n",
        "    self.data = data\n",
        "    self.column_name = column_name\n",
        "\n",
        "  def detect_outliers(self):\n",
        "\n",
        "\n",
        "    if not pd.api.types.is_numeric_dtype(self.data[self.column_name]):\n",
        "      #print(f\"Warning: Column '{self.column_name}' is not numerical. Skipping outlier detection.\")\n",
        "      return None  # Indicate non-numerical column\n",
        "\n",
        "    q1 = self.data[self.column_name].quantile(0.25)\n",
        "    q3 = self.data[self.column_name].quantile(0.75)\n",
        "    iqr = q3 - q1\n",
        "    lower_bound = q1 - 1.5 * iqr\n",
        "    upper_bound = q3 + 1.5 * iqr\n",
        "    outliers = self.data[(self.data[self.column_name] < lower_bound) | (self.data[self.column_name] > upper_bound)].index.tolist()\n",
        "    return outliers\n",
        "\n",
        "  def get_outlier_count(self):\n",
        "\n",
        "    outliers = self.detect_outliers()\n",
        "    if outliers is None:\n",
        "        return 0  # Handle case of non-numerical column\n",
        "    else:\n",
        "        return len(outliers)\n",
        "\n",
        "  def get_outlier_percentage(self):\n",
        "\n",
        "    outlier_count = self.get_outlier_count()\n",
        "    total_data_points = len(self.data)\n",
        "    if total_data_points > 0:\n",
        "        return (outlier_count / total_data_points) * 100\n",
        "    else:\n",
        "        return 0  # Handle case of empty data\n",
        "\n",
        "  def drop_outliers(self):\n",
        "\n",
        "    outliers = self.detect_outliers()\n",
        "    if outliers is None:\n",
        "        return self.data  # Return original data if column is not numerical\n",
        "    else:\n",
        "        return self.data.drop(outliers)"
      ],
      "metadata": {
        "id": "lsIvKzcSRz9G"
      },
      "execution_count": 37,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#custom Encoder\n",
        "class CustomEncoder:\n",
        "    def __init__(self):\n",
        "        self.encoding_dict = {}  # Dictionary to store encoding mappings\n",
        "        self.counter = 1  # Counter to assign integer encoding values\n",
        "\n",
        "    def fit_transform(self, series):\n",
        "        encoded_list = []  # List to store encoded values\n",
        "\n",
        "        # Count the frequency of each item in the series\n",
        "        frequency_count = series.value_counts().sort_values(ascending=False)\n",
        "\n",
        "        # Iterate through the series based on frequency count\n",
        "        for item in series:\n",
        "            if pd.isna(item):\n",
        "                encoded_list.append(item)\n",
        "            elif item not in self.encoding_dict:\n",
        "                self.encoding_dict[item] = self.counter\n",
        "                self.counter += 1\n",
        "                encoded_list.append(self.encoding_dict[item])\n",
        "            else:\n",
        "                encoded_list.append(self.encoding_dict[item])\n",
        "\n",
        "        return encoded_list, self.encoding_dict\n",
        "\n",
        "    def inverse_transform(self, encoded_list, encoding_dict):\n",
        "        reverse_dict = {v: k for k, v in encoding_dict.items()}  # Reverse the dictionary for decoding\n",
        "        return [reverse_dict[encoded_value] if encoded_value in reverse_dict else encoded_value for encoded_value in encoded_list]"
      ],
      "metadata": {
        "id": "DPN5x8uAJIg7"
      },
      "execution_count": 38,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "from sklearn.impute import KNNImputer\n",
        "\n",
        "class KNNBatchImputer(KNNImputer):\n",
        "    def __init__(self, batch_size=500, n_neighbors=5):\n",
        "        super().__init__(n_neighbors=n_neighbors)  # Inherit from KNNImputer\n",
        "        self.batch_size = batch_size\n",
        "\n",
        "    def fit_transform(self, df):\n",
        "        data_filled = pd.DataFrame()\n",
        "        num_batches = len(df) // self.batch_size + (1 if len(df) % self.batch_size != 0 else 0)\n",
        "\n",
        "        for i in range(num_batches):\n",
        "            start_idx = i * self.batch_size\n",
        "            end_idx = min((i + 1) * self.batch_size, len(df))\n",
        "            batch_data = df.iloc[start_idx:end_idx].copy()\n",
        "\n",
        "            imputer = KNNImputer(n_neighbors=self.n_neighbors)  # Create KNNImputer for each batch\n",
        "            batch_data_filled = pd.DataFrame(imputer.fit_transform(batch_data), columns=batch_data.columns)\n",
        "\n",
        "            data_filled = pd.concat([data_filled, batch_data_filled], ignore_index=True)\n",
        "\n",
        "        return data_filled\n",
        "\n",
        "# Example usage\n",
        "#df_filled = KNNImputerWithBatching(batch_size=500, n_neighbors=5).fit_transform(df.copy())  # Avoid modifying original df\n",
        "#print(df_filled.isnull().sum())  # Check for remaining missing values"
      ],
      "metadata": {
        "id": "AiHjOAk1fXJD"
      },
      "execution_count": 39,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "import pandas as pd\n",
        "from sklearn.impute import KNNImputer\n",
        "\n",
        "class KNNImputerWithCustomEncoder1:\n",
        "    def __init__(self, batch_size=500, n_neighbors=5, custom_encoder=None):\n",
        "        self.batch_size = batch_size\n",
        "        self.n_neighbors = n_neighbors\n",
        "        self.custom_encoder = custom_encoder\n",
        "        self.encoding_dicts = {}\n",
        "\n",
        "    def fit_transform(self, df):\n",
        "        data_filled = pd.DataFrame()\n",
        "        num_batches = len(df) // self.batch_size + (1 if len(df) % self.batch_size != 0 else 0)\n",
        "\n",
        "        for i in range(num_batches):\n",
        "            start_idx = i * self.batch_size\n",
        "            end_idx = min((i + 1) * self.batch_size, len(df))\n",
        "            batch_data = df.iloc[start_idx:end_idx].copy()\n",
        "\n",
        "            self._encode_unsupported_types(batch_data)\n",
        "            imputer = KNNImputer(n_neighbors=self.n_neighbors)\n",
        "            batch_data_filled = pd.DataFrame(imputer.fit_transform(batch_data), columns=batch_data.columns)\n",
        "            self._decode_unsupported_types(batch_data_filled)\n",
        "\n",
        "            data_filled = pd.concat([data_filled, batch_data_filled], ignore_index=True)\n",
        "\n",
        "        return data_filled\n",
        "\n",
        "    def _encode_unsupported_types(self, df):\n",
        "        for col in df.columns:\n",
        "            if df[col].dtype not in ('int64', 'float64'):\n",
        "                if df[col].dtype == 'object':\n",
        "                    df[col], encoding_dict = self.custom_encoder.fit_transform(df[col])\n",
        "                    self.encoding_dicts[col] = encoding_dict\n",
        "\n",
        "    def _decode_unsupported_types(self, df):\n",
        "        for col in df.columns:\n",
        "            if col in self.encoding_dicts:\n",
        "                df[col] = self.custom_encoder.inverse_transform(   np.round(np.array(df[col])).astype('int64')  , self.encoding_dicts[col])\n",
        "\n",
        "\n",
        "# Example usage:\n",
        "# custom_encoder = CustomEncoder()  # Initialize your custom encoder\n",
        "# imputer = KNNImputerWithCustomEncoder(custom_encoder=custom_encoder)\n",
        "# df_filled = imputer.fit_transform(df)\n",
        "# print(df_filled)"
      ],
      "metadata": {
        "id": "X_Gc9LYekujq"
      },
      "execution_count": 40,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def check_missing(data):\n",
        "   print( df.iloc[-3:,-2:])\n",
        "   for col in data.columns:\n",
        "      Mi = data[col].isna().sum()\n",
        "      print(f\"{Mi}  : {col}\")"
      ],
      "metadata": {
        "id": "8z-TYAXJutVD"
      },
      "execution_count": 41,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Importing Data"
      ],
      "metadata": {
        "id": "gLP7WWbwSJHw"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "url ='https://d2beiqkhq929f0.cloudfront.net/public_assets/assets/000/002/856/original/scaler_clustering.csv'\n",
        "df = pd.read_csv(url)\n",
        "df.info()"
      ],
      "metadata": {
        "id": "SbdaoEwnSFeh",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "bed25cb1-933f-47ef-b83d-f7baa2657530"
      },
      "execution_count": 42,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "<class 'pandas.core.frame.DataFrame'>\n",
            "RangeIndex: 205843 entries, 0 to 205842\n",
            "Data columns (total 7 columns):\n",
            " #   Column            Non-Null Count   Dtype  \n",
            "---  ------            --------------   -----  \n",
            " 0   Unnamed: 0        205843 non-null  int64  \n",
            " 1   company_hash      205799 non-null  object \n",
            " 2   email_hash        205843 non-null  object \n",
            " 3   orgyear           205757 non-null  float64\n",
            " 4   ctc               205843 non-null  int64  \n",
            " 5   job_position      153279 non-null  object \n",
            " 6   ctc_updated_year  205843 non-null  float64\n",
            "dtypes: float64(2), int64(2), object(3)\n",
            "memory usage: 11.0+ MB\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Renaming columns for my convenience"
      ],
      "metadata": {
        "id": "DcXa2RBgSx4m"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "df.rename(columns={\n",
        "'Unnamed: 0':'sn',\n",
        "'company_hash':'company',\n",
        "'email_hash':'email',\n",
        "'orgyear':'j_year',\n",
        "'job_position':'position',\n",
        "'ctc_updated_year':'i_year'\n",
        "},inplace=True)"
      ],
      "metadata": {
        "id": "YVz2lAJsS3YW"
      },
      "execution_count": 43,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Checking Duplicates"
      ],
      "metadata": {
        "id": "r2jsGAXOV5kX"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#checking if there are duplicate and total\n",
        "df.duplicated().sum()"
      ],
      "metadata": {
        "id": "4lCjBZjhV_Yb",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "9ea361af-532b-4e4e-fb3b-05000574ffcd"
      },
      "execution_count": 44,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "0"
            ]
          },
          "metadata": {},
          "execution_count": 44
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#No reason to remove duplicate from seprate columns\n",
        "for col in df.columns:\n",
        "   dup = df[col].duplicated().sum()\n",
        "   print(f\"{dup} :{col}\")"
      ],
      "metadata": {
        "id": "YdbXCiGrWahT",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "e1c87a1a-5b2b-4576-d636-ab2ce833c628"
      },
      "execution_count": 45,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "0 :sn\n",
            "168543 :company\n",
            "52400 :email\n",
            "205765 :j_year\n",
            "202483 :ctc\n",
            "204826 :position\n",
            "205836 :i_year\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Unique Values"
      ],
      "metadata": {
        "id": "FD611Hl0WvOL"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "df.nunique()"
      ],
      "metadata": {
        "id": "BSl3-lOAW1c4",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "824e3c72-6f08-47b8-a7fc-324a8bd731bc"
      },
      "execution_count": 46,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "sn          205843\n",
              "company      37299\n",
              "email       153443\n",
              "j_year          77\n",
              "ctc           3360\n",
              "position      1016\n",
              "i_year           7\n",
              "dtype: int64"
            ]
          },
          "metadata": {},
          "execution_count": 46
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Outlier detection and removal"
      ],
      "metadata": {
        "id": "0bdqeGcOY9Vo"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "for col in df.columns:\n",
        "  detector = IQROutlierDetector(df, col)\n",
        "  outlier_indices = detector.detect_outliers()\n",
        "  #print(\"Outlier indices:\", outlier_indices)\n",
        "\n",
        "  outlier_count = detector.get_outlier_count()\n",
        "  outlier_percentage = round(detector.get_outlier_percentage(),2)\n",
        "  if detector.get_outlier_count() > 0 :\n",
        "    print(f\"{col} Outlier count:\", outlier_count)\n",
        "    print(f\"{col} Outlier Percentage:\", outlier_percentage, \"%\")\n",
        "    df = detector.drop_outliers()"
      ],
      "metadata": {
        "id": "ZpsWToMeZEcq",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "70fad461-3fe9-4a75-92fb-73c6f3a57f51"
      },
      "execution_count": 47,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "j_year Outlier count: 7764\n",
            "j_year Outlier Percentage: 3.77 %\n",
            "ctc Outlier count: 12366\n",
            "ctc Outlier Percentage: 6.24 %\n",
            "i_year Outlier count: 2610\n",
            "i_year Outlier Percentage: 1.41 %\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Checking for Missing Values"
      ],
      "metadata": {
        "id": "I2kwX55STP5D"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#Now checking for missing values\n",
        "check_missing(df)"
      ],
      "metadata": {
        "id": "LGyj0vruTa3n",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "86028980-b69e-4301-c520-a26164054dec"
      },
      "execution_count": 48,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "       position  i_year\n",
            "205839      NaN  2020.0\n",
            "205840      NaN  2021.0\n",
            "205842      NaN  2016.0\n",
            "0  : sn\n",
            "42  : company\n",
            "0  : email\n",
            "77  : j_year\n",
            "0  : ctc\n",
            "48245  : position\n",
            "0  : i_year\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#removing missing values from company and J_year as the account is low\n",
        "df.dropna(subset= ['company','j_year'],inplace=True,axis=0)"
      ],
      "metadata": {
        "id": "9j2NwCAhTerd"
      },
      "execution_count": 49,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Now checking for missing values\n",
        "check_missing(df)"
      ],
      "metadata": {
        "id": "MPzqrMZFf7gg",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "a6093943-821d-4337-ff86-22e38dfa01d8"
      },
      "execution_count": 50,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "       position  i_year\n",
            "205839      NaN  2020.0\n",
            "205840      NaN  2021.0\n",
            "205842      NaN  2016.0\n",
            "0  : sn\n",
            "0  : company\n",
            "0  : email\n",
            "0  : j_year\n",
            "0  : ctc\n",
            "48195  : position\n",
            "0  : i_year\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Changing datatype"
      ],
      "metadata": {
        "id": "c_Y50cE6mdog"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "df['j_year']=df['j_year'].astype('int64')\n",
        "df['i_year']=df['i_year'].astype('int64')"
      ],
      "metadata": {
        "id": "IQukTxbWmho0"
      },
      "execution_count": 51,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "#input_series = df['position']\n",
        "#df['position'], pos_dict = encoder.fit_transform(input_series)\n",
        "#input_series = df['company']\n",
        "#df['company'], com_dict = encoder.fit_transform(input_series)\n",
        "#input_series = df['email']\n",
        "#df['email'], ema_dict = encoder.fit_transform(input_series)"
      ],
      "metadata": {
        "id": "zfNi0mKPmKWB"
      },
      "execution_count": 52,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "encoder = CustomEncoder()  # Create an instance of CustomEncoder\n",
        "\n",
        "imputer = KNNImputerWithCustomEncoder1(custom_encoder = encoder)\n",
        "df_filled = imputer.fit_transform(df)\n",
        "df = df_filled\n",
        "check_missing(df)"
      ],
      "metadata": {
        "id": "6yYt06lxf9wY",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "3bb1273b-bc2d-46c3-9b89-88518f4b93c7"
      },
      "execution_count": 53,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "                 position  i_year\n",
            "182981   Backend Engineer  2020.0\n",
            "182982       Data Analyst  2021.0\n",
            "182983  Frontend Engineer  2016.0\n",
            "0  : sn\n",
            "0  : company\n",
            "0  : email\n",
            "0  : j_year\n",
            "0  : ctc\n",
            "0  : position\n",
            "0  : i_year\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "df['position'].value_counts().head(25)"
      ],
      "metadata": {
        "id": "HJHJiMwG6F8V",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "ac5a7905-f7a2-4603-a7c4-c6ab94068e04"
      },
      "execution_count": 68,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "position\n",
              "Backend Engineer          45094\n",
              "FullStack Engineer        26339\n",
              "Other                     16630\n",
              "Frontend Engineer         15125\n",
              "Data Analyst               9872\n",
              "iOS Engineer               9470\n",
              "Engineering Leadership     7417\n",
              "Data Scientist             7373\n",
              "QA Engineer                6519\n",
              "Android Engineer           5888\n",
              "SDET                       4702\n",
              "Devops Engineer            4166\n",
              "Engineering Intern         4111\n",
              "Support Engineer           3425\n",
              "Research Engineers         1350\n",
              "Fullstack Engineer         1311\n",
              "Product Designer           1014\n",
              "Product Manager             880\n",
              "Backend Architect           583\n",
              "Non Coder                   560\n",
              "Database Administrator      460\n",
              "Program Manager             387\n",
              "qgjvr tzextra               350\n",
              "hzxntaytvrny sqghu          301\n",
              "mrvwpmhwp                   285\n",
              "Name: count, dtype: int64"
            ]
          },
          "metadata": {},
          "execution_count": 68
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Create an instance of the class\n",
        "categorizer = TextCategorizer()\n",
        "\n",
        "# Fit the model on the text column\n",
        "categorizer.fit(df['position'])\n",
        "\n",
        "\n",
        "# Predict categories for new text data\n",
        "df['predicted'] = categorizer.predict(df['position'])\n",
        "\n",
        "# Print the results\n",
        "print(df[['position', 'predicted']].head(20))"
      ],
      "metadata": {
        "id": "o7sxqgVJ8-p3",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 949
        },
        "outputId": "2d78ed36-5fe6-4172-e45e-777d9801ded6"
      },
      "execution_count": 69,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "                      position\n",
            "0                        Other\n",
            "1           FullStack Engineer\n",
            "2             Backend Engineer\n",
            "3             Backend Engineer\n",
            "4           FullStack Engineer\n",
            "...                        ...\n",
            "182979  Engineering Leadership\n",
            "182980            Data Analyst\n",
            "182981        Backend Engineer\n",
            "182982            Data Analyst\n",
            "182983       Frontend Engineer\n",
            "\n",
            "[182984 rows x 1 columns]\n",
            "[]\n"
          ]
        },
        {
          "output_type": "error",
          "ename": "ValueError",
          "evalue": "empty vocabulary; perhaps the documents only contain stop words",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-69-9dfe81897f4d>\u001b[0m in \u001b[0;36m<cell line: 5>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0;31m# Fit the model on the text column\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 5\u001b[0;31m \u001b[0mcategorizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdf\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'position'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      6\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      7\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-67-0cb3d0e498a2>\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, text_column)\u001b[0m\n\u001b[1;32m     37\u001b[0m         \u001b[0mprint\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mtext_data\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0miloc\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     38\u001b[0m         \u001b[0mpreprocessed_text\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtext_data\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0miloc\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mapply\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpreprocess_text\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 39\u001b[0;31m         \u001b[0mX\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvectorizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit_transform\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpreprocessed_text\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     40\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     41\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mclusters\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mkmeans\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit_predict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/sklearn/feature_extraction/text.py\u001b[0m in \u001b[0;36mfit_transform\u001b[0;34m(self, raw_documents, y)\u001b[0m\n\u001b[1;32m   2131\u001b[0m             \u001b[0msublinear_tf\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msublinear_tf\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2132\u001b[0m         )\n\u001b[0;32m-> 2133\u001b[0;31m         \u001b[0mX\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0msuper\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit_transform\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mraw_documents\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2134\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_tfidf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2135\u001b[0m         \u001b[0;31m# X is already a transformed view of raw_documents so\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/sklearn/feature_extraction/text.py\u001b[0m in \u001b[0;36mfit_transform\u001b[0;34m(self, raw_documents, y)\u001b[0m\n\u001b[1;32m   1386\u001b[0m                     \u001b[0;32mbreak\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1387\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1388\u001b[0;31m         \u001b[0mvocabulary\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mX\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_count_vocab\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mraw_documents\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfixed_vocabulary_\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1389\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1390\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbinary\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/sklearn/feature_extraction/text.py\u001b[0m in \u001b[0;36m_count_vocab\u001b[0;34m(self, raw_documents, fixed_vocab)\u001b[0m\n\u001b[1;32m   1292\u001b[0m             \u001b[0mvocabulary\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mvocabulary\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1293\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mvocabulary\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1294\u001b[0;31m                 raise ValueError(\n\u001b[0m\u001b[1;32m   1295\u001b[0m                     \u001b[0;34m\"empty vocabulary; perhaps the documents only contain stop words\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1296\u001b[0m                 )\n",
            "\u001b[0;31mValueError\u001b[0m: empty vocabulary; perhaps the documents only contain stop words"
          ]
        }
      ]
    }
  ]
}