{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyOgvjXEE0YQcLrZx2FOVh9N",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/ashishkumarsaklani/AIML/blob/main/Clustering_Pro.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#cluster project\n",
        "\n",
        "#importing Libraries"
      ],
      "metadata": {
        "id": "Vxp2A28cGt75"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 35,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "brxzXIX9GsIi",
        "outputId": "83615b57-cabc-45e2-d566-a252d3e3659a"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Package stopwords is already up-to-date!\n",
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Package punkt is already up-to-date!\n"
          ]
        }
      ],
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "\n",
        "\n",
        "\n",
        "from sklearn.impute import KNNImputer\n",
        "from sklearn.preprocessing import LabelEncoder,StandardScaler,MinMaxScaler,PolynomialFeatures\n",
        "from sklearn.metrics.pairwise import pairwise_kernels\n",
        "from sklearn.mixture import GaussianMixture\n",
        "from sklearn.cluster import KMeans,SpectralClustering\n",
        "\n",
        "\n",
        "from scipy.cluster.hierarchy import dendrogram, linkage, fcluster\n",
        "from mpl_toolkits.mplot3d import Axes3D\n",
        "import plotly.express as px\n",
        "\n",
        "from nltk.corpus import stopwords\n",
        "from nltk.tokenize import word_tokenize\n",
        "from nltk.stem import PorterStemmer\n",
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "from collections import Counter\n",
        "import nltk\n",
        "nltk.download('stopwords')\n",
        "import nltk\n",
        "nltk.download('punkt')\n",
        "\n",
        "import re\n",
        "import itertools\n",
        "import warnings\n",
        "warnings.filterwarnings('ignore')"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#custom functions"
      ],
      "metadata": {
        "id": "3Nc31-eyHRDa"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Normalisation of text string and preprocessor for nltk\n",
        "\n",
        "import pandas as pd\n",
        "import re\n",
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "from sklearn.cluster import KMeans\n",
        "from nltk.corpus import stopwords\n",
        "from nltk.stem import PorterStemmer\n",
        "from nltk.tokenize import word_tokenize\n",
        "\n",
        "class TextCategorizer:\n",
        "    def __init__(self, n_clusters=50):\n",
        "        self.n_clusters = n_clusters\n",
        "        self.stop_words = set(stopwords.words('english'))\n",
        "        self.stemmer = PorterStemmer()\n",
        "        self.vectorizer = TfidfVectorizer()\n",
        "        self.kmeans = KMeans(n_clusters=self.n_clusters)\n",
        "        self.representative_titles = {}\n",
        "\n",
        "    def preprocess_text(self, text):\n",
        "        text = text.lower()  # Convert to lowercase\n",
        "        text = re.sub(r'[^\\w\\s]', '', text)  # Remove punctuation\n",
        "        tokens = word_tokenize(text)\n",
        "        tokens = [self.stemmer.stem(token) for token in tokens if token not in self.stop_words]\n",
        "        return ' '.join(tokens)\n",
        "\n",
        "    def fit(self, text_column):\n",
        "        text_data = pd.DataFrame(text_column)\n",
        "        text_data = text_data.fillna('Not Disclosed')  # Fill NaN values\n",
        "\n",
        "        preprocessed_text = text_data[0].apply(self.preprocess_text)\n",
        "        X = self.vectorizer.fit_transform(preprocessed_text)\n",
        "\n",
        "        self.clusters = self.kmeans.fit_predict(X)\n",
        "        self.representative_titles = self._find_representative_titles(text_data[0])\n",
        "\n",
        "    def _find_representative_titles(self, text_data):\n",
        "        representative_titles = {}\n",
        "        for cluster_id in range(self.n_clusters):\n",
        "            cluster_indices = [i for i, label in enumerate(self.clusters) if label == cluster_id]\n",
        "            common_job_titles = text_data.iloc[cluster_indices].value_counts().index[0]  # Get most common title\n",
        "            representative_titles[cluster_id] = common_job_titles\n",
        "        return representative_titles\n",
        "\n",
        "    def predict(self, text_column):\n",
        "        return text_column.apply(lambda x: self._assign_clean_title(x))\n",
        "\n",
        "    def _assign_clean_title(self, text):\n",
        "        if pd.isna(text):\n",
        "            return 'Not Disclosed'\n",
        "        try:\n",
        "            cluster_id = self.clusters[list(self.vectorizer.transform([text]))[0].argmax()]\n",
        "            return self.representative_titles.get(cluster_id, 'Others')\n",
        "        except ValueError:\n",
        "            return 'Other'"
      ],
      "metadata": {
        "id": "zBRz2BZZGtA_"
      },
      "execution_count": 36,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class IQROutlierDetector:\n",
        "\n",
        "  def __init__(self, data, column_name):\n",
        "\n",
        "    self.data = data\n",
        "    self.column_name = column_name\n",
        "\n",
        "  def detect_outliers(self):\n",
        "\n",
        "\n",
        "    if not pd.api.types.is_numeric_dtype(self.data[self.column_name]):\n",
        "      #print(f\"Warning: Column '{self.column_name}' is not numerical. Skipping outlier detection.\")\n",
        "      return None  # Indicate non-numerical column\n",
        "\n",
        "    q1 = self.data[self.column_name].quantile(0.25)\n",
        "    q3 = self.data[self.column_name].quantile(0.75)\n",
        "    iqr = q3 - q1\n",
        "    lower_bound = q1 - 1.5 * iqr\n",
        "    upper_bound = q3 + 1.5 * iqr\n",
        "    outliers = self.data[(self.data[self.column_name] < lower_bound) | (self.data[self.column_name] > upper_bound)].index.tolist()\n",
        "    return outliers\n",
        "\n",
        "  def get_outlier_count(self):\n",
        "\n",
        "    outliers = self.detect_outliers()\n",
        "    if outliers is None:\n",
        "        return 0  # Handle case of non-numerical column\n",
        "    else:\n",
        "        return len(outliers)\n",
        "\n",
        "  def get_outlier_percentage(self):\n",
        "\n",
        "    outlier_count = self.get_outlier_count()\n",
        "    total_data_points = len(self.data)\n",
        "    if total_data_points > 0:\n",
        "        return (outlier_count / total_data_points) * 100\n",
        "    else:\n",
        "        return 0  # Handle case of empty data\n",
        "\n",
        "  def drop_outliers(self):\n",
        "\n",
        "    outliers = self.detect_outliers()\n",
        "    if outliers is None:\n",
        "        return self.data  # Return original data if column is not numerical\n",
        "    else:\n",
        "        return self.data.drop(outliers)"
      ],
      "metadata": {
        "id": "lsIvKzcSRz9G"
      },
      "execution_count": 37,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#custom Encoder\n",
        "class CustomEncoder:\n",
        "    def __init__(self):\n",
        "        self.encoding_dict = {}  # Dictionary to store encoding mappings\n",
        "        self.counter = 1  # Counter to assign integer encoding values\n",
        "\n",
        "    def fit_transform(self, series):\n",
        "        encoded_list = []  # List to store encoded values\n",
        "\n",
        "        # Count the frequency of each item in the series\n",
        "        frequency_count = series.value_counts().sort_values(ascending=False)\n",
        "\n",
        "        # Iterate through the series based on frequency count\n",
        "        for item in series:\n",
        "            if pd.isna(item):\n",
        "                encoded_list.append(item)\n",
        "            elif item not in self.encoding_dict:\n",
        "                self.encoding_dict[item] = self.counter\n",
        "                self.counter += 1\n",
        "                encoded_list.append(self.encoding_dict[item])\n",
        "            else:\n",
        "                encoded_list.append(self.encoding_dict[item])\n",
        "\n",
        "        return encoded_list, self.encoding_dict\n",
        "\n",
        "    def inverse_transform(self, encoded_list, encoding_dict):\n",
        "        reverse_dict = {v: k for k, v in encoding_dict.items()}  # Reverse the dictionary for decoding\n",
        "        return [reverse_dict[encoded_value] if encoded_value in reverse_dict else encoded_value for encoded_value in encoded_list]"
      ],
      "metadata": {
        "id": "DPN5x8uAJIg7"
      },
      "execution_count": 38,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "from sklearn.impute import KNNImputer\n",
        "\n",
        "class KNNBatchImputer(KNNImputer):\n",
        "    def __init__(self, batch_size=500, n_neighbors=5):\n",
        "        super().__init__(n_neighbors=n_neighbors)  # Inherit from KNNImputer\n",
        "        self.batch_size = batch_size\n",
        "\n",
        "    def fit_transform(self, df):\n",
        "        data_filled = pd.DataFrame()\n",
        "        num_batches = len(df) // self.batch_size + (1 if len(df) % self.batch_size != 0 else 0)\n",
        "\n",
        "        for i in range(num_batches):\n",
        "            start_idx = i * self.batch_size\n",
        "            end_idx = min((i + 1) * self.batch_size, len(df))\n",
        "            batch_data = df.iloc[start_idx:end_idx].copy()\n",
        "\n",
        "            imputer = KNNImputer(n_neighbors=self.n_neighbors)  # Create KNNImputer for each batch\n",
        "            batch_data_filled = pd.DataFrame(imputer.fit_transform(batch_data), columns=batch_data.columns)\n",
        "\n",
        "            data_filled = pd.concat([data_filled, batch_data_filled], ignore_index=True)\n",
        "\n",
        "        return data_filled\n",
        "\n",
        "# Example usage\n",
        "#df_filled = KNNImputerWithBatching(batch_size=500, n_neighbors=5).fit_transform(df.copy())  # Avoid modifying original df\n",
        "#print(df_filled.isnull().sum())  # Check for remaining missing values"
      ],
      "metadata": {
        "id": "AiHjOAk1fXJD"
      },
      "execution_count": 39,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "import pandas as pd\n",
        "from sklearn.impute import KNNImputer\n",
        "\n",
        "class KNNImputerWithCustomEncoder1:\n",
        "    def __init__(self, batch_size=500, n_neighbors=5, custom_encoder=None):\n",
        "        self.batch_size = batch_size\n",
        "        self.n_neighbors = n_neighbors\n",
        "        self.custom_encoder = custom_encoder\n",
        "        self.encoding_dicts = {}\n",
        "\n",
        "    def fit_transform(self, df):\n",
        "        data_filled = pd.DataFrame()\n",
        "        num_batches = len(df) // self.batch_size + (1 if len(df) % self.batch_size != 0 else 0)\n",
        "\n",
        "        for i in range(num_batches):\n",
        "            start_idx = i * self.batch_size\n",
        "            end_idx = min((i + 1) * self.batch_size, len(df))\n",
        "            batch_data = df.iloc[start_idx:end_idx].copy()\n",
        "\n",
        "            self._encode_unsupported_types(batch_data)\n",
        "            imputer = KNNImputer(n_neighbors=self.n_neighbors)\n",
        "            batch_data_filled = pd.DataFrame(imputer.fit_transform(batch_data), columns=batch_data.columns)\n",
        "            self._decode_unsupported_types(batch_data_filled)\n",
        "\n",
        "            data_filled = pd.concat([data_filled, batch_data_filled], ignore_index=True)\n",
        "\n",
        "        return data_filled\n",
        "\n",
        "    def _encode_unsupported_types(self, df):\n",
        "        for col in df.columns:\n",
        "            if df[col].dtype not in ('int64', 'float64'):\n",
        "                if df[col].dtype == 'object':\n",
        "                    df[col], encoding_dict = self.custom_encoder.fit_transform(df[col])\n",
        "                    self.encoding_dicts[col] = encoding_dict\n",
        "\n",
        "    def _decode_unsupported_types(self, df):\n",
        "        for col in df.columns:\n",
        "            if col in self.encoding_dicts:\n",
        "                df[col] = self.custom_encoder.inverse_transform(   np.round(np.array(df[col])).astype('int64')  , self.encoding_dicts[col])\n",
        "\n",
        "\n",
        "# Example usage:\n",
        "# custom_encoder = CustomEncoder()  # Initialize your custom encoder\n",
        "# imputer = KNNImputerWithCustomEncoder(custom_encoder=custom_encoder)\n",
        "# df_filled = imputer.fit_transform(df)\n",
        "# print(df_filled)"
      ],
      "metadata": {
        "id": "X_Gc9LYekujq"
      },
      "execution_count": 40,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def check_missing(data):\n",
        "   print( df.iloc[-3:,-2:])\n",
        "   for col in data.columns:\n",
        "      Mi = data[col].isna().sum()\n",
        "      print(f\"{Mi}  : {col}\")"
      ],
      "metadata": {
        "id": "8z-TYAXJutVD"
      },
      "execution_count": 121,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Importing Data"
      ],
      "metadata": {
        "id": "gLP7WWbwSJHw"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "url ='https://d2beiqkhq929f0.cloudfront.net/public_assets/assets/000/002/856/original/scaler_clustering.csv'\n",
        "df = pd.read_csv(url)\n",
        "df.info()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "SbdaoEwnSFeh",
        "outputId": "5d05ed17-8f9f-4937-abf4-3cf046a220e1"
      },
      "execution_count": 41,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "<class 'pandas.core.frame.DataFrame'>\n",
            "RangeIndex: 205843 entries, 0 to 205842\n",
            "Data columns (total 7 columns):\n",
            " #   Column            Non-Null Count   Dtype  \n",
            "---  ------            --------------   -----  \n",
            " 0   Unnamed: 0        205843 non-null  int64  \n",
            " 1   company_hash      205799 non-null  object \n",
            " 2   email_hash        205843 non-null  object \n",
            " 3   orgyear           205757 non-null  float64\n",
            " 4   ctc               205843 non-null  int64  \n",
            " 5   job_position      153279 non-null  object \n",
            " 6   ctc_updated_year  205843 non-null  float64\n",
            "dtypes: float64(2), int64(2), object(3)\n",
            "memory usage: 11.0+ MB\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Renaming columns for my convenience"
      ],
      "metadata": {
        "id": "DcXa2RBgSx4m"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "df.rename(columns={\n",
        "'Unnamed: 0':'sn',\n",
        "'company_hash':'company',\n",
        "'email_hash':'email',\n",
        "'orgyear':'j_year',\n",
        "'job_position':'position',\n",
        "'ctc_updated_year':'i_year'\n",
        "},inplace=True)"
      ],
      "metadata": {
        "id": "YVz2lAJsS3YW"
      },
      "execution_count": 42,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Checking Duplicates"
      ],
      "metadata": {
        "id": "r2jsGAXOV5kX"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#checking if there are duplicate and total\n",
        "df.duplicated().sum()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "4lCjBZjhV_Yb",
        "outputId": "dc75ff55-1bd6-4198-fb19-861179fd365b"
      },
      "execution_count": 43,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "0"
            ]
          },
          "metadata": {},
          "execution_count": 43
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#No reason to remove duplicate from seprate columns\n",
        "for col in df.columns:\n",
        "   dup = df[col].duplicated().sum()\n",
        "   print(f\"{dup} :{col}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "YdbXCiGrWahT",
        "outputId": "1083252e-1393-472b-f420-85e030aab467"
      },
      "execution_count": 117,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "0 :sn\n",
            "149189 :company\n",
            "47773 :email\n",
            "182964 :j_year\n",
            "180655 :ctc\n",
            "180956 :position\n",
            "182978 :i_year\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Unique Values"
      ],
      "metadata": {
        "id": "FD611Hl0WvOL"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "df.nunique()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "BSl3-lOAW1c4",
        "outputId": "04b3a59e-34da-4575-ed48-47f4eab1435b"
      },
      "execution_count": 45,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "sn          205843\n",
              "company      37299\n",
              "email       153443\n",
              "j_year          77\n",
              "ctc           3360\n",
              "position      1016\n",
              "i_year           7\n",
              "dtype: int64"
            ]
          },
          "metadata": {},
          "execution_count": 45
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Outlier detection and removal"
      ],
      "metadata": {
        "id": "0bdqeGcOY9Vo"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "for col in df.columns:\n",
        "  detector = IQROutlierDetector(df, col)\n",
        "  outlier_indices = detector.detect_outliers()\n",
        "  #print(\"Outlier indices:\", outlier_indices)\n",
        "\n",
        "  outlier_count = detector.get_outlier_count()\n",
        "  outlier_percentage = round(detector.get_outlier_percentage(),2)\n",
        "  if detector.get_outlier_count() > 0 :\n",
        "    print(f\"{col} Outlier count:\", outlier_count)\n",
        "    print(f\"{col} Outlier Percentage:\", outlier_percentage, \"%\")\n",
        "    df = detector.drop_outliers()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ZpsWToMeZEcq",
        "outputId": "832afb81-7f17-4b90-e2f6-682e635c1576"
      },
      "execution_count": 46,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "j_year Outlier count: 7764\n",
            "j_year Outlier Percentage: 3.77 %\n",
            "ctc Outlier count: 12366\n",
            "ctc Outlier Percentage: 6.24 %\n",
            "i_year Outlier count: 2610\n",
            "i_year Outlier Percentage: 1.41 %\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Checking for Missing Values"
      ],
      "metadata": {
        "id": "I2kwX55STP5D"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#Now checking for missing values\n",
        "check_missing(df)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "LGyj0vruTa3n",
        "outputId": "6b786c92-ad8a-4594-d4b8-b4e6ad9e2ead"
      },
      "execution_count": 122,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "                 position  i_year\n",
            "182981   Backend Engineer  2020.0\n",
            "182982       Data Analyst  2021.0\n",
            "182983  Frontend Engineer  2016.0\n",
            "0  : sn\n",
            "0  : company\n",
            "0  : email\n",
            "0  : j_year\n",
            "0  : ctc\n",
            "0  : position\n",
            "0  : i_year\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#removing missing values from company and J_year as the account is low\n",
        "df.dropna(subset= ['company','j_year'],inplace=True,axis=0)"
      ],
      "metadata": {
        "id": "9j2NwCAhTerd"
      },
      "execution_count": 107,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Now checking for missing values\n",
        "check_missing(df)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "MPzqrMZFf7gg",
        "outputId": "fd73b899-8fc7-4b81-e669-4ff4d9b5482b"
      },
      "execution_count": 120,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "              ctc           position  i_year\n",
            "182981   500000.0   Backend Engineer  2020.0\n",
            "182982   700000.0       Data Analyst  2021.0\n",
            "182983  1240000.0  Frontend Engineer  2016.0\n",
            "0  : sn\n",
            "0  : company\n",
            "0  : email\n",
            "0  : j_year\n",
            "0  : ctc\n",
            "0  : position\n",
            "0  : i_year\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Changing datatype"
      ],
      "metadata": {
        "id": "c_Y50cE6mdog"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "df['j_year']=df['j_year'].astype('int64')\n",
        "df['i_year']=df['i_year'].astype('int64')"
      ],
      "metadata": {
        "id": "IQukTxbWmho0"
      },
      "execution_count": 50,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "#input_series = df['position']\n",
        "#df['position'], pos_dict = encoder.fit_transform(input_series)\n",
        "#input_series = df['company']\n",
        "#df['company'], com_dict = encoder.fit_transform(input_series)\n",
        "#input_series = df['email']\n",
        "#df['email'], ema_dict = encoder.fit_transform(input_series)"
      ],
      "metadata": {
        "id": "zfNi0mKPmKWB"
      },
      "execution_count": 51,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "encoder = CustomEncoder()  # Create an instance of CustomEncoder\n",
        "\n",
        "imputer = KNNImputerWithCustomEncoder1(custom_encoder = encoder)\n",
        "df_filled = imputer.fit_transform(df)\n",
        "df = df_filled\n",
        "check_missing(df)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "6yYt06lxf9wY",
        "outputId": "ce79056d-9b6f-4097-8414-41cd08d7704d"
      },
      "execution_count": 105,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "              ctc           position  i_year\n",
            "182981   500000.0   Backend Engineer  2020.0\n",
            "182982   700000.0       Data Analyst  2021.0\n",
            "182983  1240000.0  Frontend Engineer  2016.0\n",
            "sn :0\n",
            "company :0\n",
            "email :0\n",
            "j_year :0\n",
            "ctc :0\n",
            "position :0\n",
            "i_year :0\n"
          ]
        }
      ]
    }
  ]
}