{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyM+WHgUQJIkGk8wZThuetRc",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/ashishkumarsaklani/AIML/blob/main/Clustering_Pro.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#cluster project\n",
        "\n",
        "#importing Libraries"
      ],
      "metadata": {
        "id": "Vxp2A28cGt75"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 66,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "brxzXIX9GsIi",
        "outputId": "5eb840b2-1bca-4aab-94ee-82735bf34a69"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Package stopwords is already up-to-date!\n",
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Package punkt is already up-to-date!\n"
          ]
        }
      ],
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "\n",
        "\n",
        "\n",
        "from sklearn.impute import KNNImputer\n",
        "from sklearn.preprocessing import LabelEncoder,StandardScaler,MinMaxScaler,PolynomialFeatures\n",
        "from sklearn.metrics.pairwise import pairwise_kernels\n",
        "from sklearn.mixture import GaussianMixture\n",
        "from sklearn.cluster import KMeans,SpectralClustering\n",
        "\n",
        "\n",
        "from scipy.cluster.hierarchy import dendrogram, linkage, fcluster\n",
        "from mpl_toolkits.mplot3d import Axes3D\n",
        "import plotly.express as px\n",
        "\n",
        "from nltk.corpus import stopwords\n",
        "from nltk.tokenize import word_tokenize\n",
        "from nltk.stem import PorterStemmer\n",
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "from collections import Counter\n",
        "import nltk\n",
        "nltk.download('stopwords')\n",
        "import nltk\n",
        "nltk.download('punkt')\n",
        "\n",
        "import re\n",
        "import itertools\n",
        "import warnings\n",
        "warnings.filterwarnings('ignore')"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#custom functions"
      ],
      "metadata": {
        "id": "3Nc31-eyHRDa"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#df[columns=1]"
      ],
      "metadata": {
        "id": "of1ZXkksEsTU"
      },
      "execution_count": 64,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "class TextCategorizer:\n",
        "    def __init__(self, cluster_count=50):\n",
        "        self.cluster_count = cluster_count\n",
        "        self.stemmer = PorterStemmer()\n",
        "        self.vectorizer = TfidfVectorizer()\n",
        "        self.kmeans = KMeans(n_clusters=self.cluster_count)\n",
        "        self.representative_titles = {}\n",
        "\n",
        "    def normalize_text(self, text):\n",
        "        # Normalizing text: correcting spelling mistakes, converting to lowercase, and standardizing casing\n",
        "        return text.lower()\n",
        "\n",
        "    def fit(self, text_column):\n",
        "        normalized_text = text_column.apply(self.normalize_text)\n",
        "        self.vectorizer.fit(normalized_text)\n",
        "        X = self.vectorizer.transform(normalized_text)\n",
        "        self.clusters = self.kmeans.fit_predict(X)\n",
        "        self.representative_titles = self._find_representative_titles(text_column)\n",
        "\n",
        "    def _find_representative_titles(self, text_column):\n",
        "        top_job_titles = list(text_column.value_counts().head(self.cluster_count * 2).index)\n",
        "        representative_titles = {}\n",
        "        for cluster_id in range(self.cluster_count):\n",
        "            cluster_indices = [i for i, label in enumerate(self.clusters) if label == cluster_id]\n",
        "            cluster_job_titles = text_column.iloc[cluster_indices]\n",
        "            # Filter out non-English or meaningless job titles\n",
        "            cluster_job_titles = cluster_job_titles[cluster_job_titles.isin(top_job_titles)]\n",
        "            # Select the most common job title in the cluster\n",
        "            common_job_title = cluster_job_titles.value_counts().index[0] if not cluster_job_titles.empty else 'Others'\n",
        "            representative_titles[cluster_id] = common_job_title\n",
        "        return representative_titles\n",
        "\n",
        "    def predict(self, text_column):\n",
        "        normalized_text = text_column.apply(self.normalize_text)\n",
        "        X = self.vectorizer.transform(normalized_text)\n",
        "        predicted_clusters = self.kmeans.predict(X)\n",
        "        return [self.representative_titles[cluster_id] for cluster_id in predicted_clusters]"
      ],
      "metadata": {
        "id": "_GnAg5BtmeAO"
      },
      "execution_count": 95,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Normalisation of text string and preprocessor for nltk\n",
        "\n",
        "#import pandas as pd\n",
        "#import re\n",
        "#from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "#from sklearn.cluster import KMeans\n",
        "#from nltk.corpus import stopwords\n",
        "#from nltk.stem import PorterStemmer\n",
        "#from nltk.tokenize import word_tokenize\n",
        "\n",
        "class TextCategorizer_clustering:\n",
        "    def __init__(self, n_clusters=90):\n",
        "        self.n_clusters = n_clusters\n",
        "        self.stop_words = set(stopwords.words('english'))\n",
        "        self.stemmer = PorterStemmer()\n",
        "        self.vectorizer = TfidfVectorizer()\n",
        "        self.kmeans = KMeans(n_clusters=self.n_clusters)\n",
        "        self.representative_titles = {}\n",
        "\n",
        "    def preprocess_text(self, text):\n",
        "        text = text.lower()  # Convert to lowercase\n",
        "        text = re.sub(r'[^\\w\\s]', '', text)  # Remove punctuation\n",
        "        tokens = word_tokenize(text)\n",
        "        tokens = [self.stemmer.stem(token) for token in tokens if token not in self.stop_words]\n",
        "\n",
        "\n",
        "        #print (tokens)# for debug\n",
        "        return ' '.join(tokens)\n",
        "\n",
        "    def fit(self, text_column):\n",
        "        text_data = pd.DataFrame(text_column)\n",
        "        text_data = text_data.fillna('Not Disclosed')  # Fill NaN values\n",
        "\n",
        "        #print(1,\"Text data:\")\n",
        "        #print(text_data.head())  # Print the first few rows of text data to verify it's correctly loaded\n",
        "\n",
        "        preprocessed_text = text_data.iloc[:, 0].apply(self.preprocess_text)\n",
        "\n",
        "        #print(2,\"Preprocessed text:\")\n",
        "        #print(preprocessed_text.head())  # Print the first few preprocessed texts to verify preprocessing is working\n",
        "\n",
        "        X = self.vectorizer.fit_transform(preprocessed_text)\n",
        "\n",
        "        #print(3,\"Vocabulary:\")\n",
        "        #print(self.vectorizer.get_feature_names()[:10])  # Print the first few feature names to verify vocabulary creation\n",
        "\n",
        "        self.clusters = self.kmeans.fit_predict(X)\n",
        "        self.representative_titles = self._find_representative_titles(text_data.iloc[:,0])\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "    def _find_representative_titles(self, text_data):\n",
        "        representative_titles = {}\n",
        "        for cluster_id in range(self.n_clusters):\n",
        "            cluster_indices = [i for i, label in enumerate(self.clusters) if label == cluster_id]\n",
        "            common_job_titles = text_data.iloc[cluster_indices].value_counts().index[0]  # Get most common title\n",
        "            representative_titles[cluster_id] = common_job_titles\n",
        "        return representative_titles\n",
        "\n",
        "    def predict(self, text_column):\n",
        "        return text_column.apply(lambda x: self._assign_clean_title(x))\n",
        "\n",
        "    def _assign_clean_title(self, text):\n",
        "        if pd.isna(text):\n",
        "            return 'Not Disclosed'\n",
        "        try:\n",
        "            cluster_id = self.clusters[list(self.vectorizer.transform([text]))[0].argmax()]\n",
        "            return self.representative_titles.get(cluster_id, 'Others')\n",
        "        except ValueError:\n",
        "            return 'Other'"
      ],
      "metadata": {
        "id": "zBRz2BZZGtA_"
      },
      "execution_count": 98,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class IQROutlierDetector:\n",
        "\n",
        "  def __init__(self, data, column_name):\n",
        "\n",
        "    self.data = data\n",
        "    self.column_name = column_name\n",
        "\n",
        "  def detect_outliers(self):\n",
        "\n",
        "\n",
        "    if not pd.api.types.is_numeric_dtype(self.data[self.column_name]):\n",
        "      #print(f\"Warning: Column '{self.column_name}' is not numerical. Skipping outlier detection.\")\n",
        "      return None  # Indicate non-numerical column\n",
        "\n",
        "    q1 = self.data[self.column_name].quantile(0.25)\n",
        "    q3 = self.data[self.column_name].quantile(0.75)\n",
        "    iqr = q3 - q1\n",
        "    lower_bound = q1 - 1.5 * iqr\n",
        "    upper_bound = q3 + 1.5 * iqr\n",
        "    outliers = self.data[(self.data[self.column_name] < lower_bound) | (self.data[self.column_name] > upper_bound)].index.tolist()\n",
        "    return outliers\n",
        "\n",
        "  def get_outlier_count(self):\n",
        "\n",
        "    outliers = self.detect_outliers()\n",
        "    if outliers is None:\n",
        "        return 0  # Handle case of non-numerical column\n",
        "    else:\n",
        "        return len(outliers)\n",
        "\n",
        "  def get_outlier_percentage(self):\n",
        "\n",
        "    outlier_count = self.get_outlier_count()\n",
        "    total_data_points = len(self.data)\n",
        "    if total_data_points > 0:\n",
        "        return (outlier_count / total_data_points) * 100\n",
        "    else:\n",
        "        return 0  # Handle case of empty data\n",
        "\n",
        "  def drop_outliers(self):\n",
        "\n",
        "    outliers = self.detect_outliers()\n",
        "    if outliers is None:\n",
        "        return self.data  # Return original data if column is not numerical\n",
        "    else:\n",
        "        return self.data.drop(outliers)"
      ],
      "metadata": {
        "id": "lsIvKzcSRz9G"
      },
      "execution_count": 37,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#custom Encoder\n",
        "class CustomEncoder:\n",
        "    def __init__(self):\n",
        "        self.encoding_dict = {}  # Dictionary to store encoding mappings\n",
        "        self.counter = 1  # Counter to assign integer encoding values\n",
        "\n",
        "    def fit_transform(self, series):\n",
        "        encoded_list = []  # List to store encoded values\n",
        "\n",
        "        # Count the frequency of each item in the series\n",
        "        frequency_count = series.value_counts().sort_values(ascending=False)\n",
        "\n",
        "        # Iterate through the series based on frequency count\n",
        "        for item in series:\n",
        "            if pd.isna(item):\n",
        "                encoded_list.append(item)\n",
        "            elif item not in self.encoding_dict:\n",
        "                self.encoding_dict[item] = self.counter\n",
        "                self.counter += 1\n",
        "                encoded_list.append(self.encoding_dict[item])\n",
        "            else:\n",
        "                encoded_list.append(self.encoding_dict[item])\n",
        "\n",
        "        return encoded_list, self.encoding_dict\n",
        "\n",
        "    def inverse_transform(self, encoded_list, encoding_dict):\n",
        "        reverse_dict = {v: k for k, v in encoding_dict.items()}  # Reverse the dictionary for decoding\n",
        "        return [reverse_dict[encoded_value] if encoded_value in reverse_dict else encoded_value for encoded_value in encoded_list]"
      ],
      "metadata": {
        "id": "DPN5x8uAJIg7"
      },
      "execution_count": 38,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "from sklearn.impute import KNNImputer\n",
        "\n",
        "class KNNBatchImputer(KNNImputer):\n",
        "    def __init__(self, batch_size=500, n_neighbors=5):\n",
        "        super().__init__(n_neighbors=n_neighbors)  # Inherit from KNNImputer\n",
        "        self.batch_size = batch_size\n",
        "\n",
        "    def fit_transform(self, df):\n",
        "        data_filled = pd.DataFrame()\n",
        "        num_batches = len(df) // self.batch_size + (1 if len(df) % self.batch_size != 0 else 0)\n",
        "\n",
        "        for i in range(num_batches):\n",
        "            start_idx = i * self.batch_size\n",
        "            end_idx = min((i + 1) * self.batch_size, len(df))\n",
        "            batch_data = df.iloc[start_idx:end_idx].copy()\n",
        "\n",
        "            imputer = KNNImputer(n_neighbors=self.n_neighbors)  # Create KNNImputer for each batch\n",
        "            batch_data_filled = pd.DataFrame(imputer.fit_transform(batch_data), columns=batch_data.columns)\n",
        "\n",
        "            data_filled = pd.concat([data_filled, batch_data_filled], ignore_index=True)\n",
        "\n",
        "        return data_filled\n",
        "\n",
        "# Example usage\n",
        "#df_filled = KNNImputerWithBatching(batch_size=500, n_neighbors=5).fit_transform(df.copy())  # Avoid modifying original df\n",
        "#print(df_filled.isnull().sum())  # Check for remaining missing values"
      ],
      "metadata": {
        "id": "AiHjOAk1fXJD"
      },
      "execution_count": 39,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "import pandas as pd\n",
        "from sklearn.impute import KNNImputer\n",
        "\n",
        "class KNNImputerWithCustomEncoder1:\n",
        "    def __init__(self, batch_size=500, n_neighbors=5, custom_encoder=None):\n",
        "        self.batch_size = batch_size\n",
        "        self.n_neighbors = n_neighbors\n",
        "        self.custom_encoder = custom_encoder\n",
        "        self.encoding_dicts = {}\n",
        "\n",
        "    def fit_transform(self, df):\n",
        "        data_filled = pd.DataFrame()\n",
        "        num_batches = len(df) // self.batch_size + (1 if len(df) % self.batch_size != 0 else 0)\n",
        "\n",
        "        for i in range(num_batches):\n",
        "            start_idx = i * self.batch_size\n",
        "            end_idx = min((i + 1) * self.batch_size, len(df))\n",
        "            batch_data = df.iloc[start_idx:end_idx].copy()\n",
        "\n",
        "            self._encode_unsupported_types(batch_data)\n",
        "            imputer = KNNImputer(n_neighbors=self.n_neighbors)\n",
        "            batch_data_filled = pd.DataFrame(imputer.fit_transform(batch_data), columns=batch_data.columns)\n",
        "            self._decode_unsupported_types(batch_data_filled)\n",
        "\n",
        "            data_filled = pd.concat([data_filled, batch_data_filled], ignore_index=True)\n",
        "\n",
        "        return data_filled\n",
        "\n",
        "    def _encode_unsupported_types(self, df):\n",
        "        for col in df.columns:\n",
        "            if df[col].dtype not in ('int64', 'float64'):\n",
        "                if df[col].dtype == 'object':\n",
        "                    df[col], encoding_dict = self.custom_encoder.fit_transform(df[col])\n",
        "                    self.encoding_dicts[col] = encoding_dict\n",
        "\n",
        "    def _decode_unsupported_types(self, df):\n",
        "        for col in df.columns:\n",
        "            if col in self.encoding_dicts:\n",
        "                df[col] = self.custom_encoder.inverse_transform(   np.round(np.array(df[col])).astype('int64')  , self.encoding_dicts[col])\n",
        "\n",
        "\n",
        "# Example usage:\n",
        "# custom_encoder = CustomEncoder()  # Initialize your custom encoder\n",
        "# imputer = KNNImputerWithCustomEncoder(custom_encoder=custom_encoder)\n",
        "# df_filled = imputer.fit_transform(df)\n",
        "# print(df_filled)"
      ],
      "metadata": {
        "id": "X_Gc9LYekujq"
      },
      "execution_count": 40,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def check_missing(data):\n",
        "   print( df.iloc[-3:,-2:])\n",
        "   for col in data.columns:\n",
        "      Mi = data[col].isna().sum()\n",
        "      print(f\"{Mi}  : {col}\")"
      ],
      "metadata": {
        "id": "8z-TYAXJutVD"
      },
      "execution_count": 41,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Importing Data"
      ],
      "metadata": {
        "id": "gLP7WWbwSJHw"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "url ='https://d2beiqkhq929f0.cloudfront.net/public_assets/assets/000/002/856/original/scaler_clustering.csv'\n",
        "df = pd.read_csv(url)\n",
        "df.info()"
      ],
      "metadata": {
        "id": "SbdaoEwnSFeh",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "bed25cb1-933f-47ef-b83d-f7baa2657530"
      },
      "execution_count": 42,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "<class 'pandas.core.frame.DataFrame'>\n",
            "RangeIndex: 205843 entries, 0 to 205842\n",
            "Data columns (total 7 columns):\n",
            " #   Column            Non-Null Count   Dtype  \n",
            "---  ------            --------------   -----  \n",
            " 0   Unnamed: 0        205843 non-null  int64  \n",
            " 1   company_hash      205799 non-null  object \n",
            " 2   email_hash        205843 non-null  object \n",
            " 3   orgyear           205757 non-null  float64\n",
            " 4   ctc               205843 non-null  int64  \n",
            " 5   job_position      153279 non-null  object \n",
            " 6   ctc_updated_year  205843 non-null  float64\n",
            "dtypes: float64(2), int64(2), object(3)\n",
            "memory usage: 11.0+ MB\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Renaming columns for my convenience"
      ],
      "metadata": {
        "id": "DcXa2RBgSx4m"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "df.rename(columns={\n",
        "'Unnamed: 0':'sn',\n",
        "'company_hash':'company',\n",
        "'email_hash':'email',\n",
        "'orgyear':'j_year',\n",
        "'job_position':'position',\n",
        "'ctc_updated_year':'i_year'\n",
        "},inplace=True)"
      ],
      "metadata": {
        "id": "YVz2lAJsS3YW"
      },
      "execution_count": 43,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Checking Duplicates"
      ],
      "metadata": {
        "id": "r2jsGAXOV5kX"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#checking if there are duplicate and total\n",
        "df.duplicated().sum()"
      ],
      "metadata": {
        "id": "4lCjBZjhV_Yb",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "9ea361af-532b-4e4e-fb3b-05000574ffcd"
      },
      "execution_count": 44,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "0"
            ]
          },
          "metadata": {},
          "execution_count": 44
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#No reason to remove duplicate from seprate columns\n",
        "for col in df.columns:\n",
        "   dup = df[col].duplicated().sum()\n",
        "   print(f\"{dup} :{col}\")"
      ],
      "metadata": {
        "id": "YdbXCiGrWahT",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "e1c87a1a-5b2b-4576-d636-ab2ce833c628"
      },
      "execution_count": 45,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "0 :sn\n",
            "168543 :company\n",
            "52400 :email\n",
            "205765 :j_year\n",
            "202483 :ctc\n",
            "204826 :position\n",
            "205836 :i_year\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Unique Values"
      ],
      "metadata": {
        "id": "FD611Hl0WvOL"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "df.nunique()"
      ],
      "metadata": {
        "id": "BSl3-lOAW1c4",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "824e3c72-6f08-47b8-a7fc-324a8bd731bc"
      },
      "execution_count": 46,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "sn          205843\n",
              "company      37299\n",
              "email       153443\n",
              "j_year          77\n",
              "ctc           3360\n",
              "position      1016\n",
              "i_year           7\n",
              "dtype: int64"
            ]
          },
          "metadata": {},
          "execution_count": 46
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Outlier detection and removal"
      ],
      "metadata": {
        "id": "0bdqeGcOY9Vo"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "for col in df.columns:\n",
        "  detector = IQROutlierDetector(df, col)\n",
        "  outlier_indices = detector.detect_outliers()\n",
        "  #print(\"Outlier indices:\", outlier_indices)\n",
        "\n",
        "  outlier_count = detector.get_outlier_count()\n",
        "  outlier_percentage = round(detector.get_outlier_percentage(),2)\n",
        "  if detector.get_outlier_count() > 0 :\n",
        "    print(f\"{col} Outlier count:\", outlier_count)\n",
        "    print(f\"{col} Outlier Percentage:\", outlier_percentage, \"%\")\n",
        "    df = detector.drop_outliers()"
      ],
      "metadata": {
        "id": "ZpsWToMeZEcq",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "70fad461-3fe9-4a75-92fb-73c6f3a57f51"
      },
      "execution_count": 47,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "j_year Outlier count: 7764\n",
            "j_year Outlier Percentage: 3.77 %\n",
            "ctc Outlier count: 12366\n",
            "ctc Outlier Percentage: 6.24 %\n",
            "i_year Outlier count: 2610\n",
            "i_year Outlier Percentage: 1.41 %\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Checking for Missing Values"
      ],
      "metadata": {
        "id": "I2kwX55STP5D"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#Now checking for missing values\n",
        "check_missing(df)"
      ],
      "metadata": {
        "id": "LGyj0vruTa3n",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "86028980-b69e-4301-c520-a26164054dec"
      },
      "execution_count": 48,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "       position  i_year\n",
            "205839      NaN  2020.0\n",
            "205840      NaN  2021.0\n",
            "205842      NaN  2016.0\n",
            "0  : sn\n",
            "42  : company\n",
            "0  : email\n",
            "77  : j_year\n",
            "0  : ctc\n",
            "48245  : position\n",
            "0  : i_year\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#removing missing values from company and J_year as the account is low\n",
        "df.dropna(subset= ['company','j_year'],inplace=True,axis=0)"
      ],
      "metadata": {
        "id": "9j2NwCAhTerd"
      },
      "execution_count": 49,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Now checking for missing values\n",
        "check_missing(df)"
      ],
      "metadata": {
        "id": "MPzqrMZFf7gg",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "a6093943-821d-4337-ff86-22e38dfa01d8"
      },
      "execution_count": 50,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "       position  i_year\n",
            "205839      NaN  2020.0\n",
            "205840      NaN  2021.0\n",
            "205842      NaN  2016.0\n",
            "0  : sn\n",
            "0  : company\n",
            "0  : email\n",
            "0  : j_year\n",
            "0  : ctc\n",
            "48195  : position\n",
            "0  : i_year\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Changing datatype"
      ],
      "metadata": {
        "id": "c_Y50cE6mdog"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "df['j_year']=df['j_year'].astype('int64')\n",
        "df['i_year']=df['i_year'].astype('int64')"
      ],
      "metadata": {
        "id": "IQukTxbWmho0"
      },
      "execution_count": 51,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "#input_series = df['position']\n",
        "#df['position'], pos_dict = encoder.fit_transform(input_series)\n",
        "#input_series = df['company']\n",
        "#df['company'], com_dict = encoder.fit_transform(input_series)\n",
        "#input_series = df['email']\n",
        "#df['email'], ema_dict = encoder.fit_transform(input_series)"
      ],
      "metadata": {
        "id": "zfNi0mKPmKWB"
      },
      "execution_count": 52,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "encoder = CustomEncoder()  # Create an instance of CustomEncoder\n",
        "\n",
        "imputer = KNNImputerWithCustomEncoder1(custom_encoder = encoder)\n",
        "df_filled = imputer.fit_transform(df)\n",
        "df = df_filled\n",
        "check_missing(df)"
      ],
      "metadata": {
        "id": "6yYt06lxf9wY",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "3bb1273b-bc2d-46c3-9b89-88518f4b93c7"
      },
      "execution_count": 53,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "                 position  i_year\n",
            "182981   Backend Engineer  2020.0\n",
            "182982       Data Analyst  2021.0\n",
            "182983  Frontend Engineer  2016.0\n",
            "0  : sn\n",
            "0  : company\n",
            "0  : email\n",
            "0  : j_year\n",
            "0  : ctc\n",
            "0  : position\n",
            "0  : i_year\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "df['position'].value_counts().head(25)"
      ],
      "metadata": {
        "id": "HJHJiMwG6F8V",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "ac5a7905-f7a2-4603-a7c4-c6ab94068e04"
      },
      "execution_count": 68,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "position\n",
              "Backend Engineer          45094\n",
              "FullStack Engineer        26339\n",
              "Other                     16630\n",
              "Frontend Engineer         15125\n",
              "Data Analyst               9872\n",
              "iOS Engineer               9470\n",
              "Engineering Leadership     7417\n",
              "Data Scientist             7373\n",
              "QA Engineer                6519\n",
              "Android Engineer           5888\n",
              "SDET                       4702\n",
              "Devops Engineer            4166\n",
              "Engineering Intern         4111\n",
              "Support Engineer           3425\n",
              "Research Engineers         1350\n",
              "Fullstack Engineer         1311\n",
              "Product Designer           1014\n",
              "Product Manager             880\n",
              "Backend Architect           583\n",
              "Non Coder                   560\n",
              "Database Administrator      460\n",
              "Program Manager             387\n",
              "qgjvr tzextra               350\n",
              "hzxntaytvrny sqghu          301\n",
              "mrvwpmhwp                   285\n",
              "Name: count, dtype: int64"
            ]
          },
          "metadata": {},
          "execution_count": 68
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Create an instance of the class\n",
        "categorizer = TextCategorizer()\n",
        "\n",
        "# Fit the model on the text column\n",
        "categorizer.fit(df['position'])\n",
        "\n",
        "\n",
        "# Predict categories for new text data\n",
        "df['predicted'] = categorizer.predict(df['position'])\n",
        "\n",
        "# Print the results\n",
        "print(df[['position', 'predicted']].head(30))"
      ],
      "metadata": {
        "id": "o7sxqgVJ8-p3",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "3cf62b7c-4abb-42e2-cf72-1e88a35a1603"
      },
      "execution_count": 97,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "                  position               predicted\n",
            "0                    Other                   Other\n",
            "1       FullStack Engineer      FullStack Engineer\n",
            "2         Backend Engineer        Backend Engineer\n",
            "3         Backend Engineer        Backend Engineer\n",
            "4       FullStack Engineer      FullStack Engineer\n",
            "5       FullStack Engineer      FullStack Engineer\n",
            "6       FullStack Engineer      FullStack Engineer\n",
            "7         Backend Engineer        Backend Engineer\n",
            "8             iOS Engineer            iOS Engineer\n",
            "9        Frontend Engineer       Frontend Engineer\n",
            "10      FullStack Engineer      FullStack Engineer\n",
            "11            iOS Engineer            iOS Engineer\n",
            "12            iOS Engineer            iOS Engineer\n",
            "13            Data Analyst            Data Analyst\n",
            "14        Backend Engineer        Backend Engineer\n",
            "15        Backend Engineer        Backend Engineer\n",
            "16                   Other                   Other\n",
            "17        Backend Engineer        Backend Engineer\n",
            "18          Data Scientist          Data Scientist\n",
            "19       Frontend Engineer       Frontend Engineer\n",
            "20  Engineering Leadership  Engineering Leadership\n",
            "21          Data Scientist          Data Scientist\n",
            "22        Backend Engineer        Backend Engineer\n",
            "23          Data Scientist          Data Scientist\n",
            "24            Data Analyst            Data Analyst\n",
            "25       Frontend Engineer       Frontend Engineer\n",
            "26      Engineering Intern      Engineering Intern\n",
            "27        Backend Engineer        Backend Engineer\n",
            "28        Backend Engineer        Backend Engineer\n",
            "29        Backend Engineer        Backend Engineer\n"
          ]
        }
      ]
    }
  ]
}